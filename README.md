# sota
State of the art on the domain of LLM Hardware accelerators

| Year | Type | Title   | Speedup | Energy efficiency |
| ---- | ---- | ------- | ------- | ----------------- |
| 2019 | FPGA | MnnFast: A Fast and Scalable System Architecture for Memory-Augmented Neural Networks | 5.4x | 4.5x |
| 2020 | FPGA | FTRANS:  Energy-Efficient Acceleration of Transformers Using FPGA | 27-81x | 8.8x |
| 2020 | FPGA | Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer.   | 14x | |
| 2021 | FPGA | An FPGA-Based Overlay Processor for Natural Language Processing   | 35x | 4x-6x |
| 2021 | FPGA | Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning   | 11-2x | |
| 2022 | FPGA | DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation | 3.8x | 4x |
| 2022 | FPGA | Hardware Acceleration of Transformer Networks using FPGAs   | 2.3x | |
| 2023 | FPGA | Transformer-OPU: An FPGA-based Overlay Processor for Transformer Networks   | 15x | |
| 2023 | FPGA | A Fast and Flexible FPGA-Based Accelerator for Natural Language Processing Neural Networks   | 2.7x | |
| 2023 | FPGA | A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE    | 12.8x | 9.2x |
| 2022 | GPU A100 | Accelerating Transformer Networks through Recomposing Softmax Layers  | 2.5x   | | 
| 2022 | GPU A100 | LightSeq2: Accelerated Training for Transformer-Based Models on GPUs  | 3x   | | 
| 2023 | GPU V100 | Inference with Reference: Lossless Acceleration of Large Language Models  | 2x   | | 
| 2022 | CPU | Exponentially Faster Language Modelling.   | 78x  | | 
| 2020 | ASIC 40nm | A3: Accelerating Attention Mechanisms in Neural Networks with Approximation  | 7x   | 11x | 
| 2021 | ASIC 40nm | ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks  | 157x   | 1265x | 
| 2021 | ASIC 40nm | SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning  | 347x/162x   | 4059x/1093x | 
| 2021 | ASIC 55nm | Sanger: A Co-Design Framework for Enabling Sparse Attention Using Reconfigurable Architecture  | 22.7x/4.64x   | | 
| 2023 | ASIC 45nm | Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention  | 168x/8.7x   | 10000x/1000x | 
| 2020 | In-memory | ATT: A Fault-Tolerant ReRAM Accelerator for Attention-based Neural Networks  |  202x  | 11x | 
| 2020 | In-memory | ReTransformer: ReRAM-based Processing-in-Memory Architecture for Transformer Acceleration  |  23x  | 1086x | 
| 2022 | In-memory | In-Memory Computing based Accelerator for Transformer Networks for Long Sequences  |  200x  | 41x | 
| 2023 | In-memory | X-Former: In-Memory Acceleration of Transformers  |  85x  |  7.5x | 
| 2023 | Flash | LLM in a flash: Efficient Large Language Model Inference with Limited Memory  | 25x/5x   | | 
